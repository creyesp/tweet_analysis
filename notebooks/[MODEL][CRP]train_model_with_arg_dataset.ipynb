{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 89782, 'negative': 26272, 'positive': 107252})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = '../data/ready/full_spanish_dataset.json'\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    dataset_raw = json.load(f)\n",
    "\n",
    "c = Counter([k['klass'] for k in dataset_raw])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    @staticmethod \n",
    "    def _processed(word):\n",
    "        tt = word.lower()\n",
    "        tt = re.sub(r'\\w*(@)\\w*', '', tt)\n",
    "        tt = re.sub(r'\\w*(RT)\\w*', '', tt, )\n",
    "        tt = re.sub(r'\\w*(#)\\w*', '', tt, )\n",
    "        tt = re.sub(r\"\\S*(\\.com|\\.ly|\\.co|\\.net|\\.org|\\.me|\\.gl)\\S*\", \"\", tt)\n",
    "        tt = re.sub(r'\\w*(jaja|kaka|jeje|jiji|juju|jojo|ajaj|jaaj)\\w*','jaja',tt)\n",
    "    #     tt = tt.translate(None, string.punctuation)\n",
    "        return tt\n",
    "    \n",
    "    def binary_class(self, dataset):\n",
    "        corpus = [self._processed(k['text']) for k in dataset if k['klass'] != 'neutral']\n",
    "        mapper = {'negative': 0, 'positive': 1}\n",
    "        target = [mapper[k['klass']] for k in dataset if k['klass'] != 'neutral']\n",
    "        \n",
    "        return corpus, target\n",
    "        \n",
    "\n",
    "    def multi_class(self, dataset):\n",
    "        corpus = [self._processed(k['text']) for k in dataset]\n",
    "        mapper = {'neutral': 0, 'negative': -1, 'positive': 1}\n",
    "        target = [mapper[k['klass']] for k in dataset]\n",
    "        return corpus, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "corpus, target = ds.binary_class(dataset_raw)\n",
    "corpus_multi, target_multi = ds.multi_class(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Aditional cleaning steps (not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "my_tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stemmer_es = SpanishStemmer('spanish')\n",
    "stemmer_porter = PorterStemmer()\n",
    "stopwords_es = stopwords.words('spanish')\n",
    "\n",
    "#punctuation to remove\n",
    "non_words = list(string.punctuation) + ['¿', '¡']\n",
    "# non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenizer(document):\n",
    "    return [token for token in my_tokenizer.tokenize(document)]\n",
    "\n",
    "def stemmer(tokens):\n",
    "    return [stemmer_es.stem(token) for token in tokens]\n",
    "\n",
    "def rm_stopwords(tokens):\n",
    "    return [k for k in tokens if k not in stopwords_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo que me hizo reir gastón, no tiene nombre...\n",
      "['lo', 'que', 'me', 'hizo', 'reir', 'gastón', 'no', 'tiene', 'nombre']\n",
      "['lo', 'que', 'me', 'hiz', 'reir', 'gaston', 'no', 'tiene', 'nombr']\n",
      "['hizo', 'reir', 'gastón', 'nombre']\n"
     ]
    }
   ],
   "source": [
    "tweet = corpus[1].lower()\n",
    "print(tweet)\n",
    "print(tokenizer(tweet))\n",
    "print(stemmer(tokenizer(tweet)))\n",
    "print(rm_stopwords(tokenizer(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to num vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing = HashingVectorizer(\n",
    "    analyzer = \"word\",\n",
    "    n_features=1000,\n",
    "#     tokenizer=tokenizer_stemmer,\n",
    "    preprocessor=None,\n",
    "    #  stop_words=stopwords.words(\"spanish\"),\n",
    "    binary=True,\n",
    "    strip_accents='ascii',\n",
    "    encoding='utf-8',\n",
    "    ngram_range=(1,3), )\n",
    "\n",
    "vectorizer = CountVectorizer(  \n",
    "    analyzer = 'word',\n",
    "#     tokenizer = tokenizer,\n",
    "    lowercase = True,\n",
    "#     stop_words = stopwords_es,\n",
    "    max_features=1000,\n",
    "    min_df = 50,\n",
    "    max_df = 1.9,\n",
    "    ngram_range=(1, 2),\n",
    "    binary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  \\\n",
    "    train_test_split(corpus, target, stratify=target, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_bin_pipeline = Pipeline(\n",
    "    steps=[('processor', clone(vectorizer)),\n",
    "           (\"clf\",  LogisticRegressionCV(class_weight='balanced', n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_bin_pipeline.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_bin_pipeline.predict(x_train)\n",
    "y_test_pred = log_bin_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645428247783634"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.636285339824003"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡vamos ! conoce la historia de las victorias del real madrid en la ucl: \n",
      "probability: [[0.32236145 0.67763855]], class: [1]\n"
     ]
    }
   ],
   "source": [
    "tweet = x_test[2]\n",
    "print(tweet)\n",
    "print(f'probability: {log_bin_pipeline.predict_proba([tweet])}, class: {log_bin_pipeline.predict([tweet])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  \\\n",
    "    train_test_split(corpus_multi, target_multi, stratify=target_multi, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_multi_pipeline = Pipeline(\n",
    "    steps=[('processor', vectorizer),\n",
    "           ('clf', LogisticRegressionCV(class_weight='balanced', n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_multi_pipeline.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_multi_pipeline.predict(x_train)\n",
    "y_test_pred = log_multi_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5148059828485704"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5044332990013882"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he aceptado el reto de  y reto a     aceptáis? \n",
      "probability: [[0.20936523 0.51258017 0.27805461]], class: [0]\n"
     ]
    }
   ],
   "source": [
    "tweet = x_test[2]\n",
    "print(tweet)\n",
    "print(f'probability: {log_multi_pipeline.predict_proba([tweet])}, class: {log_multi_pipeline.predict([tweet])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the hiperparameters (Very simple because of memory issues)\n",
    "# hiperparam_pipeline_log_reg = {  \n",
    "#    \"regresion__fit_intercept\":[True], # Data is not centered\n",
    "#    \"regresion__cv\":[10, 20],\n",
    "#    \"regresion__max_iter\": [500, 600]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_grid_search = GridSearchCV(estimator=log_pipeline,\n",
    "#                               param_grid=hiperparam_pipeline_log_reg,\n",
    "#                               scoring=\"roc_auc\",\n",
    "#                               cv=10,\n",
    "#                               n_jobs=-1\n",
    "#                              )\n",
    "        \n",
    "\n",
    "# #We train the model\n",
    "# log_grid_search.fit( x_train, y_train)\n",
    "\n",
    "# #This will take a very long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_grid_search.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/linear/log_multi_pipeline.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '../models/linear/'\n",
    "\n",
    "binary_file = os.path.join(model_path, 'log_bin_pipeline.joblib')\n",
    "joblib.dump(log_bin_pipeline, binary_file)\n",
    "\n",
    "multi_file = os.path.join(model_path, 'log_multi_pipeline.joblib')\n",
    "joblib.dump(log_multi_pipeline, multi_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
