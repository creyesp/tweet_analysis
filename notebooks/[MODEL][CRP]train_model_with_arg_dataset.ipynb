{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 89782, 'negative': 26272, 'positive': 107252})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = '../data/ready/full_spanish_dataset.json'\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    dataset_raw = json.load(f)\n",
    "\n",
    "c = Counter([k['klass'] for k in dataset_raw])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '4.Denme una mano con RT para exigir q @lanacioncom retire la foto y respete el derecho a la intimidad de @MariaviicToriia su mamá y hermano.',\n",
       " 'klass': 'neutral',\n",
       " 'id_annotator': '87',\n",
       " 'id': '332473940712751104'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    @staticmethod \n",
    "    def _remove_users(speech):\n",
    "        return re.sub(r'\\w*(@)\\w*', '', speech)\n",
    "    \n",
    "    @staticmethod \n",
    "    def _remove_url(speech):\n",
    "        return re.sub(r\"\\S*(\\.com|\\.ly|\\.co|\\.net|\\.org|\\.me|\\.gl)\\S*\", \"\", speech)\n",
    "    \n",
    "    @staticmethod \n",
    "    def _remove_punctuation(speech):\n",
    "        return re.sub(r'[^\\w\\s]', '', speech)\n",
    "    \n",
    "    @staticmethod \n",
    "    def _remove_hashtag(speech):\n",
    "        return re.sub(r'\\w*(#)\\w*', '', speech)\n",
    "    \n",
    "    @staticmethod \n",
    "    def _reduce_laugh(speech):\n",
    "        return re.sub(r'\\w*(jaja|kaka|jeje|jiji|juju|jojo|ajaj|jaaj)\\w*','jaja',speech)\n",
    "    \n",
    "    def _processed(self, speech):\n",
    "        tt = speech.lower()\n",
    "        tt = self._remove_users(tt)\n",
    "        tt = self._remove_url(tt)\n",
    "        tt = self._remove_hashtag(tt)\n",
    "        tt = self._reduce_laugh(tt)\n",
    "        tt = self._remove_punctuation(tt)\n",
    "        tt = unidecode(tt)\n",
    "        return tt\n",
    "    \n",
    "    def binary_class(self, dataset, processed=True):\n",
    "        corpus = [self._processed(k['text']) if processed else k['text'] for k in dataset if k['klass'] != 'neutral']\n",
    "        mapper = {'negative': 0, 'positive': 1}\n",
    "        target = [mapper[k['klass']] for k in dataset if k['klass'] != 'neutral']\n",
    "        \n",
    "        return corpus, target\n",
    "        \n",
    "\n",
    "    def multi_class(self, dataset, processed=True):\n",
    "        corpus = [self._processed(k['text']) if processed else k['text'] for k in dataset]\n",
    "        mapper = {'neutral': 0, 'negative': -1, 'positive': 1}\n",
    "        target = [mapper[k['klass']] for k in dataset]\n",
    "        return corpus, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(speech):\n",
    "    tt = speech.lower()\n",
    "    tt = re.sub(r'\\w*(@)\\w*', '', tt)\n",
    "    tt = re.sub(r'\\w*(RT)\\w*', '', tt, )\n",
    "    tt = re.sub(r'\\w*(#)\\w*', '', tt, )\n",
    "    tt = re.sub(r\"\\S*(\\.com|\\.ly|\\.co|\\.net|\\.org|\\.me|\\.gl)\\S*\", \"\", tt)\n",
    "    tt = re.sub(r'\\w*(jaja|kaka|jeje|jiji|juju|jojo|ajaj|jaaj)\\w*','jaja',tt)\n",
    "    tt = re.sub(r'[^\\w\\s]', '', tt)\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "corpus, target = ds.binary_class(dataset_raw, processed=False)\n",
    "corpus_multi, target_multi = ds.multi_class(dataset_raw, processed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Aditional cleaning steps (not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "my_tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stemmer_es = SpanishStemmer('spanish')\n",
    "stemmer_porter = PorterStemmer()\n",
    "stopwords_es = stopwords.words('spanish')\n",
    "\n",
    "#punctuation to remove\n",
    "non_words = list(string.punctuation) + ['¿', '¡']\n",
    "# non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenizer(document):\n",
    "    return [token for token in my_tokenizer.tokenize(document) if token.isalpha()]\n",
    "\n",
    "def stemmer(tokens):\n",
    "    return [stemmer_es.stem(token) for token in tokens]\n",
    "\n",
    "def stemmer_tokenizer(speech):\n",
    "    return stemmer(tokenizer(tweet))\n",
    "\n",
    "def rm_stopwords(tokens):\n",
    "    return [k for k in tokens if k not in stopwords_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo que me hizo reir gastón, no tiene nombre...\n",
      "['lo', 'que', 'me', 'hizo', 'reir', 'gastón', 'no', 'tiene', 'nombre']\n",
      "['lo', 'que', 'me', 'hiz', 'reir', 'gaston', 'no', 'tiene', 'nombr']\n",
      "['hizo', 'reir', 'gastón', 'nombre']\n"
     ]
    }
   ],
   "source": [
    "tweet = corpus[1].lower()\n",
    "print(tweet)\n",
    "print(tokenizer(tweet))\n",
    "print(stemmer(tokenizer(tweet)))\n",
    "print(rm_stopwords(tokenizer(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'el', 'los', 'hombres', 'iran', 'a', 'la', 'luna']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('en el 20001 los hombres iran a la luna'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to num vector\n",
    "BOW = Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing = HashingVectorizer(\n",
    "    analyzer = \"word\",\n",
    "    n_features=1000,\n",
    "#     tokenizer=tokenizer_stemmer,\n",
    "    preprocessor=None,\n",
    "    #  stop_words=stopwords.words(\"spanish\"),\n",
    "    binary=True,\n",
    "    strip_accents='ascii',\n",
    "    encoding='utf-8',\n",
    "    ngram_range=(1,3), )\n",
    "\n",
    "vectorizer = CountVectorizer(  \n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenizer,\n",
    "    strip_accents='unicode',\n",
    "    preprocessor=cleaner,\n",
    "    lowercase = True,\n",
    "    stop_words = stopwords_es,\n",
    "    max_features=5000,\n",
    "#     min_df = 0.,\n",
    "#     max_df = 1.9,\n",
    "    ngram_range=(1, 3),\n",
    "    binary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  \\\n",
    "    train_test_split(corpus, target, stratify=target, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_bin_pipeline = Pipeline(\n",
    "    steps=[('processor', clone(vectorizer)),\n",
    "           (\"clf\",  LogisticRegressionCV(class_weight='balanced', n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/software/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/cesar/software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "log_bin_pipeline.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_bin_pipeline.predict(x_train)\n",
    "y_test_pred = log_bin_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6893249328303017"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486051301254446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13821,  7197],\n",
       "       [25989, 59812]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2914,  2340],\n",
       "       [ 7044, 14407]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Vamos #APorLaDécima! Conoce la historia de las victorias del Real Madrid en la UCL: bit.ly/1i10lZRpic.twitter.com/omTDc1chu5\n",
      "probability: [[0.01770089 0.98229911]], class: [1]\n"
     ]
    }
   ],
   "source": [
    "tweet = x_test[2]\n",
    "print(tweet)\n",
    "print(f'probability: {log_bin_pipeline.predict_proba([tweet])}, class: {log_bin_pipeline.predict([tweet])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  \\\n",
    "    train_test_split(corpus_multi, target_multi, stratify=target_multi, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_multi_pipeline = Pipeline(\n",
    "    steps=[('processor', clone(vectorizer)),\n",
    "           ('clf', LogisticRegressionCV(class_weight='balanced', n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/cesar/software/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/cesar/software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "log_multi_pipeline.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_multi_pipeline.predict(x_train)\n",
    "y_test_pred = log_multi_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5585969861848145"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5253907124624961"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9680,  4670,  6668],\n",
       "       [10516, 41217, 20092],\n",
       "       [15856, 21052, 48893]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9680,  4670,  6668],\n",
       "       [10516, 41217, 20092],\n",
       "       [15856, 21052, 48893]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.46      0.34     21018\n",
      "           0       0.62      0.57      0.59     71825\n",
      "           1       0.65      0.57      0.61     85801\n",
      "\n",
      "    accuracy                           0.56    178644\n",
      "   macro avg       0.51      0.53      0.51    178644\n",
      "weighted avg       0.59      0.56      0.57    178644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SetteSettamen @Conpdepau ya veo ya.... 😡😡😡😡 a ti tambien sette que te tiro un botellin a la cabeza eh ajajjajaja\n",
      "probability: [[0.51373624 0.1025064  0.38375735]], class: [-1]\n"
     ]
    }
   ],
   "source": [
    "tweet = x_test[3]\n",
    "print(tweet)\n",
    "print(f'probability: {log_multi_pipeline.predict_proba([tweet])}, class: {log_multi_pipeline.predict([tweet])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION**: We need to pay mora atention to vocabulary to improve the performance. there are a lot of word with the same meaning, pluran and singular, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abajo',\n",
       " 'abandona',\n",
       " 'abc',\n",
       " 'abierta',\n",
       " 'abierta capilla',\n",
       " 'abierta capilla ardiente',\n",
       " 'abiertas',\n",
       " 'abierto',\n",
       " 'abogado',\n",
       " 'aborto']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_multi_pipeline.steps[0][1].get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the hiperparameters (Very simple because of memory issues)\n",
    "# hiperparam_pipeline_log_reg = {  \n",
    "#    \"regresion__fit_intercept\":[True], # Data is not centered\n",
    "#    \"regresion__cv\":[10, 20],\n",
    "#    \"regresion__max_iter\": [500, 600]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_grid_search = GridSearchCV(estimator=log_pipeline,\n",
    "#                               param_grid=hiperparam_pipeline_log_reg,\n",
    "#                               scoring=\"roc_auc\",\n",
    "#                               cv=10,\n",
    "#                               n_jobs=-1\n",
    "#                              )\n",
    "        \n",
    "\n",
    "# #We train the model\n",
    "# log_grid_search.fit( x_train, y_train)\n",
    "\n",
    "# #This will take a very long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_grid_search.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/linear/log_multi_pipeline.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '../models/linear/'\n",
    "\n",
    "binary_file = os.path.join(model_path, 'log_bin_pipeline.joblib')\n",
    "joblib.dump(log_bin_pipeline, binary_file)\n",
    "\n",
    "multi_file = os.path.join(model_path, 'log_multi_pipeline.joblib')\n",
    "joblib.dump(log_multi_pipeline, multi_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
